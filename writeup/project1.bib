
% A bib file is just a collection of entries that look like the one.
% shown below. This is a citation for our course textbook formatted in 
% the BibTeX format. The easiest way to obtain these BibTeX citations 
% is to Google the title of the paper you're looking for, along with 
% the word "bibtex". The top hits are usually pages like CiteSeer,
% CiteULike and Bibsonomy that will then give you the
% option of exporting the citation for that paper in the BibTeX format.
% Simply copy-paste the citation you find into this file. To
% cite a specific paper, just use its key inside the \cite{} command 
% in the appropriate place. I often modify the key that comes with the
% downloaded citation into something simpler that is easier to work with.

@book{aima,
    author = {Russell, Stuart J. and Norvig, Peter},
    citeulike-article-id = {709475},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=773294},
    isbn = {0137903952},
    keywords = {ai, books},
    posted-at = {2006-08-09 11:54:13},
    priority = {2},
    publisher = {Pearson Education},
    title = {{Artificial Intelligence: A Modern Approach}},
    url = {http://portal.acm.org/citation.cfm?id=773294},
    year = {2003}
}
@article{auer,
 author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Fischer, Paul},
 title = {Finite-time Analysis of the Multiarmed Bandit Problem},
 journal = {Mach. Learn.},
 issue_date = {May-June 2002},
 volume = {47},
 number = {2-3},
 month = may,
 year = {2002},
 issn = {0885-6125},
 pages = {235--256},
 numpages = {22},
 url = {http://dx.doi.org/10.1023/A:1013689704352},
 doi = {10.1023/A:1013689704352},
 acmid = {599677},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {adaptive allocation rules, bandit problems, finite horizon regret},
}

@article{bubeck,
  author    = {S{\'{e}}bastien Bubeck and
               Nicol{\`{o}} Cesa{-}Bianchi},
  title     = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit
               Problems},
  journal   = {CoRR},
  volume    = {abs/1204.5721},
  year      = {2012},
  url       = {http://arxiv.org/abs/1204.5721},
  timestamp = {Wed, 10 Oct 2012 21:28:51 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1204-5721},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{agrawal,
     jstor_articletype = {research-article},
     title = {Sample Mean Based Index Policies with O(log n) Regret for the Multi-Armed Bandit Problem},
     author = {Agrawal, Rajeev},
     journal = {Advances in Applied Probability},
     jstor_issuetitle = {},
     volume = {27},
     number = {4},
     jstor_formatteddate = {Dec., 1995},
     pages = {pp. 1054-1078},
     url = {http://www.jstor.org/stable/1427934},
     ISSN = {00018678},
     abstract = {We consider a non-Bayesian infinite horizon version of the multi-armed bandit problem with the objective of designing simple policies whose regret increases slowly with time. In their seminal work on this problem, Lai and Robbins had obtained a O(log n) lower bound on the regret with a constant that depends on the Kullback-Leibler number. They also constructed policies for some specific families of probability distributions (including exponential families) that achieved the lower bound. In this paper we construct index policies that depend on the rewards from each arm only through their sample mean. These policies are computationally much simpler and are also applicable much more generally. They achieve a O(log n) regret with a constant that is also based on the Kullback-Leibler number. This constant turns out to be optimal for one-parameter exponential families; however, in general it is derived from the optimal one via a 'contraction' principle. Our results rely entirely on a few key lemmas from the theory of large deviations.},
     language = {English},
     year = {1995},
     publisher = {Applied Probability Trust},
     copyright = {Copyright Â© 1995 Applied Probability Trust},
    }

% Place additional BibTeX entries here...
